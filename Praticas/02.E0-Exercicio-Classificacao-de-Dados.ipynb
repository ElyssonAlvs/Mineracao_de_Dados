{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QXD0178 - Mineração de Dados\n",
    "# Classificação de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista de Exercícios: Classificação de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta lista de exercícios, você explorará a aplicação de métodos de aprendizado de máquina para realizar tarefas de classificação de dados. Você usará a base de dados [Food choices: College students' food and cooking preferences](https://www.kaggle.com/datasets/borapajo/food-choices?select=food_coded.csv) e avaliará vários algoritmos de classificação para determinar sua eficácia. O objetivo é entender como diferentes métodos de aprendizado de máquina se comportam em relação à acurácia na classificação de dados.\n",
    "\n",
    "O exercício será dividido em várias etapas:\n",
    "\n",
    "1. **Pré-processamento dos dados:**\n",
    "   - Descreva brevemente o conjunto de dados   \n",
    "   - Limpe o conjunto de dados, tratando valores ausentes, removendo duplicatas e realizando transformações necessárias. \n",
    "   - Caso você use os dados pré-processados na lista anterior, faça um breve descritivo dos principais ajustes.\n",
    "   - Codifique variáveis categóricas, se necessário, para que possam ser utilizadas em algoritmos de aprendizado de máquina.\n",
    "   - Cria a coluna `self_perception_overweight` com valor: `True` se a coluna `self_perception_weight` tem valor 4 ou 5; e `False`, caso contrário.\n",
    "   - Remova a coluna `self_perception_weight` do conjunto de dados.\n",
    "2. **Divisão do conjunto de dados:**\n",
    "   - Divida o conjunto de dados em um conjunto de treinamento e um conjunto de teste para avaliar o desempenho dos algoritmos. \n",
    "   - O mesmo conjunto de teste deve ser usado por todos os algoritmos analizados e nenhum dado deste pode ser usado na fase de treinamento.\n",
    "   - O atributo alvo (*rótulo*) da classificação será o campo `self_perception_overweight`.   \n",
    "3. **Seleção de algoritmos de classificação:**\n",
    "   - Selecione uma variedade de algoritmos de aprendizado de máquina para testar na tarefa de classificação.   \n",
    "   - Sua seleção deve conter, no mínimo, os seguintes métodos: Naive Bayes, k-Nearest Neighbors, Support Vector Machine (Linear/RBF), Decision Trees, Random Forest, Multilayer Perceptron.\n",
    "   - Descreva brevemente como funciona cada algoritmo selecionado.\n",
    "4. **Treinamento e avaliação:**\n",
    "   - Treine os algoritmos de classificação usando todo o conjunto de treinamento. \n",
    "   - Avalie o desempenho de cada algoritmo no conjunto de teste usando métricas como acurácia, precisão, recall e F1-score.\n",
    "   - Repita a análise treinando os algoritmos com validação cruzada.\n",
    "   - Repita a análise realizando ajuste de hiperparâmetros.\n",
    "5. **Análise dos resultados:**\n",
    "   - Prepare um texto que descreva os resultados obtidos e faça uma análise crítica destes resultados.\n",
    "   - Compare o desempenho dos diferentes algoritmos e explique por que alguns apresentaram resultados mais adequados que outros.\n",
    "   \n",
    "Documente todas as etapas em um arquivo Jupyter Notebook (`.ipynb`) que inclua as análises, o código e as justificativas. Lembre-se de que é fundamental justificar todas as decisões tomadas ao longo do processo e documentar as análises de forma clara e concisa. Este trabalho tem como objetivo proporcionar uma compreensão prática da seleção e avaliação de algoritmos de classificação em cenários de aprendizado supervisionado.\n",
    "\n",
    "Envie seu Jupyter Notebook até a data de entrega especificada nesta tarefa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solução\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de dados é um conjunto de dados que coleta informações sobre as escolhas alimentares e preferências culinárias de estudantes universitários, ela tem informações desde dados demográficos como sexo e renda, passando por dados relacionados a preferências alimentares como comida favorita, estimativa de quantas calorias possui determinado elemento e etc. As colunas presentes na base de dados são: *'GPA', 'Gender', 'breakfast', 'calories_chicken', 'calories_day','calories_scone', 'coffee', 'comfort_food', 'comfort_food_reasons', 'comfort_food_reasons_coded', 'cook', 'comfort_food_reasons_coded.1', 'cuisine', 'diet_current', 'diet_current_coded', 'drink', 'eating_changes', 'eating_changes_coded', 'eating_changes_coded1', 'eating_out', 'employment', 'ethnic_food', 'exercise', 'father_education', 'father_profession', 'fav_cuisine', 'fav_cuisine_coded', 'fav_food', 'food_childhood', 'fries', 'fruit_day', 'grade_level', 'greek_food', 'healthy_feeling', 'healthy_meal', 'ideal_diet', 'ideal_diet_coded', 'income', 'indian_food', 'italian_food', 'life_rewarding', 'marital_status', 'meals_dinner_friend', 'mother_education', 'mother_profession', 'nutritional_check', 'on_off_campus', 'parents_cook', 'pay_meal_out', 'persian_food', 'self_perception_weight', 'soup', 'sports', 'thai_food', 'tortilla_calories', 'turkey_calories', 'type_sports', 'veggies_day', 'vitamins', 'waffle_calories', 'weight'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na parte de **Limpeza dos Dados**, parecido com oque foi feito na primeira limpeza do dataset utilizando apenas as bibliotecas numpy e pandas, foi realizada apenas a adapatação de algumas operações para que fossem executadas em uma única célula, a substituição de valores NaN e alguns que não faziam sentido no contexto da coluna localizada na base de dados para valores que fizessem sentido e não gerasse resultados muito tendenciosos. No caso de valores númericos, foram utilizadas estratégias como, por exemplo, a substituição por valores de média e moda. Em valores de string, foram substituídos NaN por uma única constante 'none', considerada como valor vazio. Além disso, para outras strings que não faziam sentido dentro de seu contexto como 'personal' onde esperava-se um resultado, usar outra unidade de peso etc. foi implementado mais uma vez o valor none. Por último, também houve a exclusão de linhas inteiras cujo a maioria dos valores ou até todos eles eram NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:94: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:105: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:94: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:105: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:94: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df['weight'] = df['weight'].str.extract('(\\d+)').astype(float)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:105: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df['GPA'] = df['GPA'].str.extract('(\\d+)').astype(float)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['soup'].fillna(df['soup'].mode()[0], inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['sports'].fillna(df['sports'].mode()[0], inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['calories_day'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['tortilla_calories'].fillna(df['tortilla_calories'].mode()[0], inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['calories_scone'].fillna(df['calories_scone'].mode()[0], inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['cook'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['cuisine'].fillna(df['cuisine'].mode()[0], inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['drink'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['employment'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['exercise'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['father_education'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:55: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['fav_food'].fillna(df['fav_food'].mode()[0], inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:60: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['income'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:65: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['life_rewarding'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:69: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['marital_status'].fillna(mode, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['mother_education'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:78: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['on_off_campus'].fillna(df['on_off_campus'].mode()[0], inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:83: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['persian_food'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:88: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['self_perception_weight'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:98: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['weight'].fillna(media_rounded, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:107: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['GPA'].fillna(media, inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['eating_changes'].fillna('none', inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:114: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['father_profession'].fillna('none', inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:115: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['meals_dinner_friend'].fillna('none', inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:116: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['mother_profession'].fillna('none', inplace=True)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\2104592061.py:117: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['type_sports'].fillna('none', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/She-Codes-Now/Intro-to-Data-Science-with-R/master/food_coded.csv\")\n",
    "# Preenche valores ausentes na coluna 'soup' com a moda\n",
    "df['soup'].fillna(df['soup'].mode()[0], inplace=True)\n",
    "\n",
    "# Preenche valores ausentes na coluna 'sports' com a moda\n",
    "df['sports'].fillna(df['sports'].mode()[0], inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'calories_day' e preenche valores ausentes com a média arredondada\n",
    "media = df['calories_day'].mean()\n",
    "media_rounded = round(media, 1)\n",
    "df['calories_day'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Preenche valores ausentes nas colunas 'tortilla_calories' e 'calories_scone' com a moda\n",
    "df['tortilla_calories'].fillna(df['tortilla_calories'].mode()[0], inplace=True)\n",
    "df['calories_scone'].fillna(df['calories_scone'].mode()[0], inplace=True)\n",
    "\n",
    "# Remove a coluna 'comfort_food_reasons_coded'\n",
    "del df['comfort_food_reasons_coded']\n",
    "\n",
    "# Renomeia a coluna 'comfort_food_reasons_coded.1' para 'comfort_food_reasons_coded'\n",
    "df.rename(columns={'comfort_food_reasons_coded.1': 'comfort_food_reasons_coded'}, inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'cook' e preenche valores ausentes com a média arredondada\n",
    "media = df['cook'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['cook'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Preenche valores ausentes na coluna 'cuisine' com a moda\n",
    "df['cuisine'].fillna(df['cuisine'].mode()[0], inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'drink' e preenche valores ausentes com a média arredondada\n",
    "media = df['drink'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['drink'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'employment' e preenche valores ausentes com a média arredondada\n",
    "media = df['employment'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['employment'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'exercise' e preenche valores ausentes com a média arredondada\n",
    "media = df['exercise'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['exercise'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'father_education' e preenche valores ausentes com a média arredondada\n",
    "media = df['father_education'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['father_education'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Preenche valores ausentes na coluna 'fav_food' com a moda\n",
    "df['fav_food'].fillna(df['fav_food'].mode()[0], inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'income' e preenche valores ausentes com a média arredondada\n",
    "media = df['income'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['income'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'life_rewarding' e preenche valores ausentes com a média arredondada\n",
    "media = df['life_rewarding'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['life_rewarding'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Preenche valores ausentes na coluna 'marital_status' com a moda\n",
    "mode = df['marital_status'].mode()[0]\n",
    "df['marital_status'].fillna(mode, inplace=True)\n",
    "df.at[74, 'marital_status'] = mode\n",
    "\n",
    "# Calcula a média da coluna 'mother_education' e preenche valores ausentes com a média arredondada\n",
    "media = df['mother_education'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['mother_education'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Preenche valores ausentes na coluna 'on_off_campus' com a moda\n",
    "df['on_off_campus'].fillna(df['on_off_campus'].mode()[0], inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'persian_food' e preenche valores ausentes com a média arredondada\n",
    "media = df['persian_food'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['persian_food'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Calcula a média da coluna 'self_perception_weight' e preenche valores ausentes com a média arredondada\n",
    "media = df['self_perception_weight'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df['self_perception_weight'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Define o valor da linha 2 na coluna 'weight' como NaN\n",
    "df.at[2, 'weight'] = np.nan\n",
    "\n",
    "# Extrai valores numéricos da coluna 'weight', converte para float e preenche valores ausentes com a média arredondada\n",
    "df['weight'] = df['weight'].str.extract('(\\d+)').astype(float)\n",
    "media = df['weight'].mean()\n",
    "media_rounded = round(media, 0)\n",
    "df.at[2, 'weight'] = media_rounded\n",
    "df['weight'].fillna(media_rounded, inplace=True)\n",
    "\n",
    "# Define valores NaN nas linhas 61 e 104 da coluna 'GPA'\n",
    "df.at[61, 'GPA'] = np.nan\n",
    "df.at[104, 'GPA'] = np.nan\n",
    "\n",
    "# Extrai valores numéricos da coluna 'GPA', converte para float e preenche valores ausentes com a média\n",
    "df['GPA'] = df['GPA'].str.extract('(\\d+)').astype(float)\n",
    "media = df['GPA'].mean()\n",
    "df['GPA'].fillna(media, inplace=True)\n",
    "\n",
    "# Remove a linha 74 do DataFrame e redefine os índices\n",
    "df = df.drop(74).reset_index(drop=True)\n",
    "\n",
    "# Preenche valores ausentes em várias colunas categóricas com a string 'none'\n",
    "df['eating_changes'].fillna('none', inplace=True)\n",
    "df['father_profession'].fillna('none', inplace=True)\n",
    "df['meals_dinner_friend'].fillna('none', inplace=True)\n",
    "df['mother_profession'].fillna('none', inplace=True)\n",
    "df['type_sports'].fillna('none', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\343034208.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
      "C:\\Users\\elyss\\AppData\\Local\\Temp\\ipykernel_18088\\343034208.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace(\" \", \"\") if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# Colocando todas as strings em minúsculas\n",
    "df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "# Tirando os espaços em branco\n",
    "df = df.applymap(lambda x: x.replace(\" \", \"\") if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "foi criada uma tabela que representa se o estudante pratica ou não esportes. Isso ajudará o algoritmo pois há a tendência natural que praticar esportes tende à dificultar a existência de sobrepeso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      0\n",
       "3      0\n",
       "4      1\n",
       "      ..\n",
       "119    1\n",
       "120    1\n",
       "121    0\n",
       "122    0\n",
       "123    0\n",
       "Name: practice_sports, Length: 124, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['practice_sports'] = np.where(df['type_sports'] != 'none', 1, 0)\n",
    "df['practice_sports']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"criar categorias númericas para os tipos de culinária favoritos dos estudantes?\". Pode haver uma tendência de determinada culinária ocasionar uma sensação de sobrepeso. Então a biblioteca LabelEncoder transforma valores categóricos em valores numéricos, mapeando cada valor categórico único em 'fav_cuisine' para um número inteiro, começando de 0 até n_classes-1, sendo o n a quantidade de categorias únicas, e assim armazenado em uma nova coluna que nela contém os valores numéricos correspondentes às categorias originais, apenas exibindo as colunas com valores transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6\n",
       "1      23\n",
       "2      23\n",
       "3      45\n",
       "4      23\n",
       "       ..\n",
       "119    23\n",
       "120    36\n",
       "121    30\n",
       "122    23\n",
       "123    17\n",
       "Name: fav_cuisine_cat, Length: 124, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['fav_cuisine_cat'] = le.fit_transform(df['fav_cuisine'])\n",
    "df['fav_cuisine_cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As comidas que mais aparecem na tabela *comfort_food*, a quantidade 10 oi um valor considerável para o tamanho da base de dados, então, criando-se uma nova coluna numérica que verifica se aquele estudante tem como comida \"confortável\" plo menos 1 dessas comidas, se for confirmado que o estudante tem uma comida favorita, com tendência, tem chances de ser acima do peso, sendo um bom parâmetro para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "119    1\n",
       "120    0\n",
       "121    0\n",
       "122    1\n",
       "123    1\n",
       "Name: got_popular_comfort_food, Length: 124, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Concatena todos os valores da coluna 'comfort_food' em uma única string e divide em uma lista de alimentos\n",
    "foods = ','.join(df['comfort_food']).split(',')\n",
    "\n",
    "# Conta a frequência de cada alimento na lista\n",
    "couting_foods = Counter(foods)\n",
    "\n",
    "# Cria uma lista de alimentos que aparecem 10 ou mais vezes\n",
    "foods_more_than_10 = [food for food, counting in couting_foods.items() if counting >= 10]\n",
    "\n",
    "# Exibe a lista de alimentos populares\n",
    "foods_more_than_10\n",
    "\n",
    "# Define uma função para verificar se um registro contém alimentos populares\n",
    "def find_food(s):\n",
    "    foods = s.split(',')\n",
    "    for food in foods:\n",
    "        if food in foods_more_than_10:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Aplica a função 'find_food' a cada registro da coluna 'comfort_food' e cria uma nova coluna 'got_popular_comfort_food'\n",
    "df['got_popular_comfort_food'] = df['comfort_food'].apply(find_food)\n",
    "\n",
    "# Exibe a nova coluna com os valores 1 ou 0\n",
    "df['got_popular_comfort_food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mesmo processo para a *food_childhood*, a infância costuma influênciar na tendência de saúde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "119    0\n",
       "120    0\n",
       "121    0\n",
       "122    0\n",
       "123    0\n",
       "Name: got_popular_food_childhood, Length: 124, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "foods = ','.join(df['food_childhood']).split(',')\n",
    "couting_foods = Counter(foods)\n",
    "foods_more_than_10 = [food for food, counting in couting_foods.items() if counting >= 10]\n",
    "foods_more_than_10\n",
    "\n",
    "def find_food(s):\n",
    "    foods = s.split(',')\n",
    "    for food in foods:\n",
    "        if food in foods_more_than_10:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "df['got_popular_food_childhood'] = df['food_childhood'].apply(find_food)\n",
    "df['got_popular_food_childhood']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remover os dados aberto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 124 entries, 0 to 123\n",
      "Data columns (total 64 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   GPA                         124 non-null    float64\n",
      " 1   Gender                      124 non-null    int64  \n",
      " 2   breakfast                   124 non-null    int64  \n",
      " 3   calories_chicken            124 non-null    int64  \n",
      " 4   calories_day                124 non-null    float64\n",
      " 5   calories_scone              124 non-null    float64\n",
      " 6   coffee                      124 non-null    int64  \n",
      " 7   comfort_food                124 non-null    object \n",
      " 8   comfort_food_reasons        123 non-null    object \n",
      " 9   cook                        124 non-null    float64\n",
      " 10  comfort_food_reasons_coded  124 non-null    int64  \n",
      " 11  cuisine                     124 non-null    float64\n",
      " 12  diet_current                124 non-null    object \n",
      " 13  diet_current_coded          124 non-null    int64  \n",
      " 14  drink                       124 non-null    float64\n",
      " 15  eating_changes              124 non-null    object \n",
      " 16  eating_changes_coded        124 non-null    int64  \n",
      " 17  eating_changes_coded1       124 non-null    int64  \n",
      " 18  eating_out                  124 non-null    int64  \n",
      " 19  employment                  124 non-null    float64\n",
      " 20  ethnic_food                 124 non-null    int64  \n",
      " 21  exercise                    124 non-null    float64\n",
      " 22  father_education            124 non-null    float64\n",
      " 23  father_profession           124 non-null    object \n",
      " 24  fav_cuisine                 123 non-null    object \n",
      " 25  fav_cuisine_coded           124 non-null    int64  \n",
      " 26  fav_food                    124 non-null    float64\n",
      " 27  food_childhood              124 non-null    object \n",
      " 28  fries                       124 non-null    int64  \n",
      " 29  fruit_day                   124 non-null    int64  \n",
      " 30  grade_level                 124 non-null    int64  \n",
      " 31  greek_food                  124 non-null    int64  \n",
      " 32  healthy_feeling             124 non-null    int64  \n",
      " 33  healthy_meal                124 non-null    object \n",
      " 34  ideal_diet                  124 non-null    object \n",
      " 35  ideal_diet_coded            124 non-null    int64  \n",
      " 36  income                      124 non-null    float64\n",
      " 37  indian_food                 124 non-null    int64  \n",
      " 38  italian_food                124 non-null    int64  \n",
      " 39  life_rewarding              124 non-null    float64\n",
      " 40  marital_status              124 non-null    float64\n",
      " 41  meals_dinner_friend         124 non-null    object \n",
      " 42  mother_education            124 non-null    float64\n",
      " 43  mother_profession           124 non-null    object \n",
      " 44  nutritional_check           124 non-null    int64  \n",
      " 45  on_off_campus               124 non-null    float64\n",
      " 46  parents_cook                124 non-null    int64  \n",
      " 47  pay_meal_out                124 non-null    int64  \n",
      " 48  persian_food                124 non-null    float64\n",
      " 49  self_perception_weight      124 non-null    float64\n",
      " 50  soup                        124 non-null    float64\n",
      " 51  sports                      124 non-null    float64\n",
      " 52  thai_food                   124 non-null    int64  \n",
      " 53  tortilla_calories           124 non-null    float64\n",
      " 54  turkey_calories             124 non-null    int64  \n",
      " 55  type_sports                 124 non-null    object \n",
      " 56  veggies_day                 124 non-null    int64  \n",
      " 57  vitamins                    124 non-null    int64  \n",
      " 58  waffle_calories             124 non-null    int64  \n",
      " 59  weight                      124 non-null    float64\n",
      " 60  practice_sports             124 non-null    int64  \n",
      " 61  fav_cuisine_cat             124 non-null    int64  \n",
      " 62  got_popular_comfort_food    124 non-null    int64  \n",
      " 63  got_popular_food_childhood  124 non-null    int64  \n",
      "dtypes: float64(21), int64(31), object(12)\n",
      "memory usage: 62.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('comfort_food', axis=1, inplace=True)\n",
    "df.drop('comfort_food_reasons', axis=1, inplace=True)\n",
    "df.drop('diet_current', axis=1, inplace=True)\n",
    "df.drop('eating_changes', axis=1, inplace=True)\n",
    "df.drop('father_profession', axis=1, inplace=True)\n",
    "df.drop('fav_cuisine', axis=1, inplace=True)\n",
    "df.drop('food_childhood', axis=1, inplace=True)\n",
    "df.drop('healthy_meal', axis=1, inplace=True)\n",
    "df.drop('ideal_diet', axis=1, inplace=True)\n",
    "df.drop('meals_dinner_friend', axis=1, inplace=True)\n",
    "df.drop('mother_profession', axis=1, inplace=True)\n",
    "df.drop('type_sports', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criar uma colina *self_perception_overweight* para remover a *self_perception_weight*, percepção acima do peso e a do peso normal, para se ter uma abordagem preditiva e prever comportamentos alimentares ou riscos de doenças por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma nova coluna 'self_perception_overweight' que é True se 'self_perception_weight' for 4.0 ou 5.0, indicando percepção de sobrepeso\n",
    "df['self_perception_overweight'] = (df['self_perception_weight'] == 4.0) | (df['self_perception_weight'] == 5.0)\n",
    "\n",
    "# Remove a coluna original 'self_perception_weight' do DataFrame\n",
    "df.drop('self_perception_weight', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão do conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui foi dividido o dataset final para o treinamento dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a variável alvo 'y' como a coluna 'self_perception_overweight'\n",
    "y = df['self_perception_overweight']\n",
    "\n",
    "# Define as variáveis preditoras 'X' removendo a coluna 'self_perception_overweight'\n",
    "X = df.drop(columns=['self_perception_overweight'])\n",
    "\n",
    "# Divide os dados em conjuntos de treino e teste, com 40% dos dados para teste e uma semente aleatória para reprodutibilidade\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verifico se o *split* foi feito corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões de X_train: (74, 51)\n",
      "Dimensões de X_test: (50, 51)\n",
      "Dimensões de y_train: (74,)\n",
      "Dimensões de y_test: (50,)\n",
      "\n",
      "Informações de X_train:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 74 entries, 91 to 37\n",
      "Data columns (total 51 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   GPA                         74 non-null     float64\n",
      " 1   Gender                      74 non-null     int64  \n",
      " 2   breakfast                   74 non-null     int64  \n",
      " 3   calories_chicken            74 non-null     int64  \n",
      " 4   calories_day                74 non-null     float64\n",
      " 5   calories_scone              74 non-null     float64\n",
      " 6   coffee                      74 non-null     int64  \n",
      " 7   cook                        74 non-null     float64\n",
      " 8   comfort_food_reasons_coded  74 non-null     int64  \n",
      " 9   cuisine                     74 non-null     float64\n",
      " 10  diet_current_coded          74 non-null     int64  \n",
      " 11  drink                       74 non-null     float64\n",
      " 12  eating_changes_coded        74 non-null     int64  \n",
      " 13  eating_changes_coded1       74 non-null     int64  \n",
      " 14  eating_out                  74 non-null     int64  \n",
      " 15  employment                  74 non-null     float64\n",
      " 16  ethnic_food                 74 non-null     int64  \n",
      " 17  exercise                    74 non-null     float64\n",
      " 18  father_education            74 non-null     float64\n",
      " 19  fav_cuisine_coded           74 non-null     int64  \n",
      " 20  fav_food                    74 non-null     float64\n",
      " 21  fries                       74 non-null     int64  \n",
      " 22  fruit_day                   74 non-null     int64  \n",
      " 23  grade_level                 74 non-null     int64  \n",
      " 24  greek_food                  74 non-null     int64  \n",
      " 25  healthy_feeling             74 non-null     int64  \n",
      " 26  ideal_diet_coded            74 non-null     int64  \n",
      " 27  income                      74 non-null     float64\n",
      " 28  indian_food                 74 non-null     int64  \n",
      " 29  italian_food                74 non-null     int64  \n",
      " 30  life_rewarding              74 non-null     float64\n",
      " 31  marital_status              74 non-null     float64\n",
      " 32  mother_education            74 non-null     float64\n",
      " 33  nutritional_check           74 non-null     int64  \n",
      " 34  on_off_campus               74 non-null     float64\n",
      " 35  parents_cook                74 non-null     int64  \n",
      " 36  pay_meal_out                74 non-null     int64  \n",
      " 37  persian_food                74 non-null     float64\n",
      " 38  soup                        74 non-null     float64\n",
      " 39  sports                      74 non-null     float64\n",
      " 40  thai_food                   74 non-null     int64  \n",
      " 41  tortilla_calories           74 non-null     float64\n",
      " 42  turkey_calories             74 non-null     int64  \n",
      " 43  veggies_day                 74 non-null     int64  \n",
      " 44  vitamins                    74 non-null     int64  \n",
      " 45  waffle_calories             74 non-null     int64  \n",
      " 46  weight                      74 non-null     float64\n",
      " 47  practice_sports             74 non-null     int64  \n",
      " 48  fav_cuisine_cat             74 non-null     int64  \n",
      " 49  got_popular_comfort_food    74 non-null     int64  \n",
      " 50  got_popular_food_childhood  74 non-null     int64  \n",
      "dtypes: float64(20), int64(31)\n",
      "memory usage: 30.1 KB\n",
      "None\n",
      "\n",
      "Informações de X_test:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50 entries, 48 to 74\n",
      "Data columns (total 51 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   GPA                         50 non-null     float64\n",
      " 1   Gender                      50 non-null     int64  \n",
      " 2   breakfast                   50 non-null     int64  \n",
      " 3   calories_chicken            50 non-null     int64  \n",
      " 4   calories_day                50 non-null     float64\n",
      " 5   calories_scone              50 non-null     float64\n",
      " 6   coffee                      50 non-null     int64  \n",
      " 7   cook                        50 non-null     float64\n",
      " 8   comfort_food_reasons_coded  50 non-null     int64  \n",
      " 9   cuisine                     50 non-null     float64\n",
      " 10  diet_current_coded          50 non-null     int64  \n",
      " 11  drink                       50 non-null     float64\n",
      " 12  eating_changes_coded        50 non-null     int64  \n",
      " 13  eating_changes_coded1       50 non-null     int64  \n",
      " 14  eating_out                  50 non-null     int64  \n",
      " 15  employment                  50 non-null     float64\n",
      " 16  ethnic_food                 50 non-null     int64  \n",
      " 17  exercise                    50 non-null     float64\n",
      " 18  father_education            50 non-null     float64\n",
      " 19  fav_cuisine_coded           50 non-null     int64  \n",
      " 20  fav_food                    50 non-null     float64\n",
      " 21  fries                       50 non-null     int64  \n",
      " 22  fruit_day                   50 non-null     int64  \n",
      " 23  grade_level                 50 non-null     int64  \n",
      " 24  greek_food                  50 non-null     int64  \n",
      " 25  healthy_feeling             50 non-null     int64  \n",
      " 26  ideal_diet_coded            50 non-null     int64  \n",
      " 27  income                      50 non-null     float64\n",
      " 28  indian_food                 50 non-null     int64  \n",
      " 29  italian_food                50 non-null     int64  \n",
      " 30  life_rewarding              50 non-null     float64\n",
      " 31  marital_status              50 non-null     float64\n",
      " 32  mother_education            50 non-null     float64\n",
      " 33  nutritional_check           50 non-null     int64  \n",
      " 34  on_off_campus               50 non-null     float64\n",
      " 35  parents_cook                50 non-null     int64  \n",
      " 36  pay_meal_out                50 non-null     int64  \n",
      " 37  persian_food                50 non-null     float64\n",
      " 38  soup                        50 non-null     float64\n",
      " 39  sports                      50 non-null     float64\n",
      " 40  thai_food                   50 non-null     int64  \n",
      " 41  tortilla_calories           50 non-null     float64\n",
      " 42  turkey_calories             50 non-null     int64  \n",
      " 43  veggies_day                 50 non-null     int64  \n",
      " 44  vitamins                    50 non-null     int64  \n",
      " 45  waffle_calories             50 non-null     int64  \n",
      " 46  weight                      50 non-null     float64\n",
      " 47  practice_sports             50 non-null     int64  \n",
      " 48  fav_cuisine_cat             50 non-null     int64  \n",
      " 49  got_popular_comfort_food    50 non-null     int64  \n",
      " 50  got_popular_food_childhood  50 non-null     int64  \n",
      "dtypes: float64(20), int64(31)\n",
      "memory usage: 20.3 KB\n",
      "None\n",
      "\n",
      "Informações de y_train:\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 74 entries, 91 to 37\n",
      "Series name: self_perception_overweight\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "74 non-null     bool \n",
      "dtypes: bool(1)\n",
      "memory usage: 666.0 bytes\n",
      "None\n",
      "\n",
      "Informações de y_test:\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 50 entries, 48 to 74\n",
      "Series name: self_perception_overweight\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "50 non-null     bool \n",
      "dtypes: bool(1)\n",
      "memory usage: 450.0 bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# X_train: Conjunto de dados de treino para as variáveis preditoras\n",
    "# X_test: Conjunto de dados de teste para as variáveis preditoras\n",
    "# y_train: Conjunto de dados de treino para a variável alvo\n",
    "# y_test: Conjunto de dados de teste para a variável alvo\n",
    "print(\"Dimensões de X_train:\", X_train.shape)\n",
    "print(\"Dimensões de X_test:\", X_test.shape)\n",
    "print(\"Dimensões de y_train:\", y_train.shape)\n",
    "print(\"Dimensões de y_test:\", y_test.shape)\n",
    "\n",
    "print(\"\\nInformações de X_train:\")\n",
    "print(X_train.info())\n",
    "\n",
    "print(\"\\nInformações de X_test:\")\n",
    "print(X_test.info())\n",
    "\n",
    "print(\"\\nInformações de y_train:\")\n",
    "print(y_train.info())\n",
    "\n",
    "print(\"\\nInformações de y_test:\")\n",
    "print(y_test.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção de algoritmos de classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Naive Bayes**\n",
    "Algoritmo probabilístico que se baseia no teorema de Bayes para realizar tarefas de classificação. Ele assume que as caracteristicas de um determinado dado são independentes umas das outras. É rotineiramente usado para classificar dados de diversos tipos, como numéricos, categóricos e mistos.\n",
    "\n",
    "**K-Nearest Neighbors**\n",
    "Algoritmo que classifica uma nova observação com base nas observações mais próximas dela no espaço de atributos. Esse número de observações sendo especificado pelo hiperparâmetro K.\n",
    "\n",
    "**Support Vector Machine**\n",
    "Algoritmo que consiste em encontrar um hiperplano de separação ótimo entre duas classes em um espaço de atributos. É amplamente utilizado em tarefas de classificação binária, onde o objetivo é separar as observações em duas classes distintas.\n",
    "\n",
    "**Decision Tree**\n",
    "Algoritmo que classifica dados criando uma árvore de decisão. A árvore é construída dividindo o conjunto de dados em subconjuntos cada vez menores, com base nos valores das características.\n",
    "\n",
    "**Random Forest**\n",
    "Algoritmo que combina várias árvores de decisão. Isso ajuda o modelo a generalizar melhor para dados novos, reduzindo o risco de sobreajuste\n",
    "\n",
    "**Multilayer Perceptron**\n",
    "Algoritmo que usa uma rede neural artificial para classificar dados. Essa rede neural é composta por várias camadas de neurônios, que são conectados uns aos outros por pesos. A rede neural aprende a classificar dados ajustando os pesos dos neurônios.\n",
    "\n",
    "**Regressão Logística**\n",
    "Algoritmo que classifica dados usando uma função logística. Tal função é uma função matemática que mapeia um intervalo de números reais para o intervalo ente 0 e 1.Para classificar uma nova observação, o algoritmo de regressão logística calcula a probabilidade de a observação pertencer a cada classe. A observação é então classificada na classe com a probabilidade mais alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento e avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48\n",
      "Precision: 0.35294117647058826\n",
      "Recall: 0.75\n",
      "F1 Score: 0.48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Cria um modelo de classificação Naive Bayes Multinomial\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 0.1875\n",
      "F1 Score: 0.24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['LOKY_MAX_CPU_COUNT'] = '4'\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Cria um modelo KNN com 5 vizinhos\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n",
      "Precision: 0.4666666666666667\n",
      "Recall: 0.4375\n",
      "F1 Score: 0.45161290322580644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Cria um modelo de SVM com kernel linear\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68\n",
      "Precision: 0.5\n",
      "Recall: 0.375\n",
      "F1 Score: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Cria um modelo de árvore de decisão com critério de impureza de Gini e sem limite de profundidade\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=None)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "Precision: 1.0\n",
      "Recall: 0.0625\n",
      "F1 Score: 0.11764705882352941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Número de árvores na floresta = 100; critério de divisão = gini; profundidade máxima das árvores = ilimitada\n",
    "model = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None)\n",
    "# Treina o modelo com os dados de treino\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "Precision: 0.5714285714285714\n",
      "Recall: 0.25\n",
      "F1 Score: 0.34782608695652173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Número de neurônios na primeira camada oculta = 100; número de neurônios na segunda camada oculta = 50; função de ativação = ReLU; otimizador = Adam; número máximo de iterações = 1000\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o primeiro contato, temos a opção de fazer uma **Validação Cruzada** e testar os **Hiperparâmentros**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validação Cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5080645161290323\n",
      "Precision: 0.32857142857142857\n",
      "Recall: 0.6216216216216216\n",
      "F1 Score: 0.42990654205607476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Faz a validação cruzada com 5 partições -> onde cada partição é usada uma vez como conjunto de teste enquanto as outras 4 são usadas como conjunto de treino\n",
    "y_pred = cross_val_predict(model, X, y, cv=5)  \n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6209677419354839\n",
      "Precision: 0.1875\n",
      "Recall: 0.08108108108108109\n",
      "F1 Score: 0.11320754716981132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# KNN com 5 vizinhos\n",
    "model = KNeighborsClassifier(n_neighbors=5)  \n",
    "\n",
    "y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6290322580645161\n",
      "Precision: 0.38461538461538464\n",
      "Recall: 0.40540540540540543\n",
      "F1 Score: 0.39473684210526316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# SVM com kernel linear e parâmetro de regularização C=1.0\n",
    "model = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6774193548387096\n",
      "Precision: 0.46153846153846156\n",
      "Recall: 0.4864864864864865\n",
      "F1 Score: 0.47368421052631576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Árvore de decisão com critério de impureza de Gini e sem limite de profundidade\n",
    "model = DecisionTreeClassifier(criterion='gini', max_depth=None) \n",
    "\n",
    "y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7016129032258065\n",
      "Precision: 0.5\n",
      "Recall: 0.05405405405405406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Floresta aleatória com 100 árvores, critério de impureza de Gini e sem limite de profundidade\n",
    "model = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None)\n",
    "\n",
    "y_pred = cross_val_predict(model, X, y, cv=5)  \n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.5483870967741935\n",
      "Precisão: 0.24324324324324326\n",
      "Recall: 0.24324324324324326\n",
      "F1-Score: 0.24324324324324326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Rede neural com 100 neurônios na primeira camada oculta, 50 neurônios na segunda camada oculta, função de ativação ReLU e número máximo de iterações 1000\n",
    "model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', max_iter=1000)  \n",
    "\n",
    "y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "precision = precision_score(y, y_pred)\n",
    "recall = recall_score(y, y_pred)\n",
    "f1 = f1_score(y, y_pred)\n",
    "\n",
    "print(\"Acurácia:\", accuracy)\n",
    "print(\"Precisão:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui é modificado os hiperparâmetros no intuito de fazer testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48\n",
      "Precision: 0.35294117647058826\n",
      "Recall: 0.75\n",
      "F1 Score: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Para o modelo de Naive Bayes Multinomial, não temos possibilidades de hiperparâmetros para ajustar\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n",
      "Precision: 0.42857142857142855\n",
      "Recall: 0.1875\n",
      "F1 Score: 0.2608695652173913\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Cria um modelo KNN com 15 vizinhos\n",
    "model = KNeighborsClassifier(n_neighbors=15) \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n",
      "Precision: 0.4666666666666667\n",
      "Recall: 0.4375\n",
      "F1 Score: 0.45161290322580644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Parâmetro de regularização C=10.0 tolerando valores na formação da margem\n",
    "model = SVC(kernel='linear', C=10.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n",
      "Precision: 0.3125\n",
      "Recall: 0.3125\n",
      "F1 Score: 0.3125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Critério de impureza de entropia e profundidade máxima de 5\n",
    "model = DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "Precision: 1.0\n",
      "Recall: 0.0625\n",
      "F1 Score: 0.11764705882352941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Número de árvores na floresta = 200; profundidade máxima das árvores = ilimitada\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=None)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n",
      "Precision: 0.16666666666666666\n",
      "Recall: 0.0625\n",
      "F1 Score: 0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Duas camadas ocultas com 150 e 75 neurônios, função de ativação ReLU e máximo de 1000 iterações\n",
    "model = MLPClassifier(hidden_layer_sizes=(150, 75), activation='relu', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após as operações, nessa fase vamos analisar e discutir os resultados antes do parâmetros modificados:\n",
    "\n",
    "**Naive Bayes:**\n",
    "- Acurácia: 0.48\n",
    "- Precisão: 0.35\n",
    "- Recall: 0.75\n",
    "- F1-Score: 0.48\n",
    "\n",
    "O Naive Bayes apresentou uma alta taxa de recall, indicando que ele é bom em identificar positivos corretos, mas a acurácia e a precisão foram relativamente baixas.\n",
    "\n",
    "**K-Nearest Neighbors:**\n",
    "- Acurácia: 0.62\n",
    "- Precisão: 0.33\n",
    "- Recall: 0.19\n",
    "- F1-Score: 0.24\n",
    "\n",
    "O KNN teve uma acurácia moderada, mas a precisão e o recall foram baixos, sugerindo dificuldades em lidar com falsos positivos e verdadeiros positivos.\n",
    "\n",
    "**Support Vector Machine:**\n",
    "- Acurácia: 0.66\n",
    "- Precisão: 0.47\n",
    "- Recall: 0.44\n",
    "- F1-Score: 0.45\n",
    "\n",
    "O SVM teve um desempenho moderado, com uma acurácia razoável, mas o recall mais baixo indica dificuldades em recuperar todos os positivos verdadeiros.\n",
    "\n",
    "**Decision Tree:**\n",
    "- Acurácia: 0.68\n",
    "- Precisão: 0.50\n",
    "- Recall: 0.44\n",
    "- F1-Score: 0.47\n",
    "\n",
    "A Decision Tree obteve uma acurácia intermediária, mas a precisão e o recall foram relativamente baixos, sugerindo problemas de equilíbrio entre falsos positivos e verdadeiros positivos.\n",
    "\n",
    "**Random Forest:**\n",
    "- Acurácia: 0.72\n",
    "- Precisão: 1.00\n",
    "- Recall: 0.13\n",
    "- F1-Score: 0.22\n",
    "\n",
    "O Random Forest teve a maior acurácia, mas a precisão muito alta e o recall muito baixo indicam um possível sobreajuste aos dados de treinamento.\n",
    "\n",
    "**Multilayer Perceptron:**\n",
    "- Acurácia: 0.66\n",
    "- Precisão: 0.35\n",
    "- Recall: 0.16\n",
    "- F1-Score: 0.22\n",
    "\n",
    "O Multilayer Perceptron teve uma acurácia intermediária, mas tanto a precisão quanto o recall foram baixos, sugerindo dificuldades em equilibrar falsos positivos e verdadeiros positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "com as mudanças dos hiperparâmetros:\n",
    "\n",
    "**Naive Bayes:**\n",
    "- Acurácia: 0.48\n",
    "- Precisão: 0.35\n",
    "- Recall: 0.75\n",
    "- F1-Score: 0.48\n",
    "\n",
    "Os resultados do Naive Bayes permaneceram os mesmos, pois não há hiperparâmetros para ajustar.\n",
    "\n",
    "**K-Nearest Neighbors:**\n",
    "- Acurácia: 0.60\n",
    "- Precisão: 0.17\n",
    "- Recall: 0.06\n",
    "- F1-Score: 0.09\n",
    "\n",
    "A mudança no número de vizinhos para 15 resultou em uma queda significativa na precisão, recall e F1-Score, indicando que o modelo teve dificuldades em identificar corretamente as classes.\n",
    "\n",
    "**Support Vector Machine:**\n",
    "- Acurácia: 0.60\n",
    "- Precisão: 0.17\n",
    "- Recall: 0.06\n",
    "- F1-Score: 0.09\n",
    "\n",
    "Aumentar o parâmetro de regularização C para 10.0 não melhorou o desempenho do SVM, resultando em uma baixa precisão e recall.\n",
    "\n",
    "**Decision Tree:**\n",
    "- Acurácia: 0.60\n",
    "- Precisão: 0.17\n",
    "- Recall: 0.06\n",
    "- F1-Score: 0.09\n",
    "\n",
    "A alteração do critério de impureza para entropia e a limitação da profundidade máxima para 5 resultaram em um desempenho pior, com baixa precisão e recall.\n",
    "\n",
    "**Random Forest:**\n",
    "- Acurácia: 0.60\n",
    "- Precisão: 0.17\n",
    "- Recall: 0.06\n",
    "- F1-Score: 0.09\n",
    "\n",
    "Aumentar o número de árvores para 200 não trouxe melhorias significativas, resultando em uma baixa precisão e recall.\n",
    "\n",
    "**Multilayer Perceptron:**\n",
    "- Acurácia: 0.60\n",
    "- Precisão: 0.17\n",
    "- Recall: 0.06\n",
    "- F1-Score: 0.09\n",
    "\n",
    "A mudança na arquitetura da rede neural para duas camadas ocultas com 150 e 75 neurônios também não melhorou o desempenho, resultando em baixa precisão e recall.\n",
    "Em resumo, as mudanças nos hiperparâmetros não trouxeram melhorias significativas para os modelos, e em alguns casos, pioraram o desempenho.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao compararmos a complexidade do Random Forest e do MLP, podemos ter uma boa noção do desempenho dos dois. Percebemos que o conjunto de dados tinha uma boa quantidade de valores a se trabalhar para chegar à conclusão esperada. Sendo assim, a lógica do Random Forest pode ter trabalhado de maneira mais concisa ou até mais adequada frente à base de dados, enquanto a lógica do MLP pode ter se perdido na manipulação dos dados, resultando em valores baixos em recall e precisão.\n",
    "\n",
    "Relativo ao KNN, percebemos uma acurácia moderada para baixa enquanto todos os outros indicadores tiveram um desempenho relativamente baixo. Isso pode ser justificado pela grande quantidade de características existentes na base de dados, resultando em uma distribuição desequilibrada das classes que o algoritmo usa para trabalhar.\n",
    "\n",
    "A acurácia baixa do Naive Bayes pode ter se dado à simplicidade exacerbada do algoritmo. Isso, aliado à distribuição majoritariamente numérica do formato de dados (o que geralmente vai contra a natureza do algoritmo, que é mais utilizado para dados categóricos), pode indicar o porquê ele não conseguiu trabalhar muito bem os dados para chegar no resultado esperado.\n",
    "\n",
    "No caso da Decision Tree, seu desempenho relativamente baixo pode ter sido dado ao fato dos conjuntos de dados terem muitos valores e não terem uma ligação direta entre si para chegar ao resultado esperado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
